@incollection{abuzainaSphereDetectionKinect2013,
  title = {Sphere {{Detection}} in {{Kinect Point Clouds}} via the {{3D Hough Transform}}},
  booktitle = {Computer {{Analysis}} of {{Images}} and {{Patterns}}},
  author = {Abuzaina, Anas and Nixon, Mark S. and Carter, John N.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wilson, Richard and Hancock, Edwin and Bors, Adrian and Smith, William},
  year = {2013},
  volume = {8048},
  pages = {290--297},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40246-3_36},
  urldate = {2024-11-22},
  abstract = {We introduce a fast, robust and accurate Hough Transform (HT) based algorithm for detecting spherical structures in 3D point clouds. To our knowledge, our algorithm is the first HT based implementation that detects spherical structures in typical in 3D point clouds generated by consumer depth sensors such as the Microsoft Kinect. Our approach has been designed to be computationally efficient; reducing an established limitation of HT based approaches. We provide experimental analysis of the achieved results, showing a robust performance against occlusion, and we show superior performance to the only other HT based algorithm for detecting spheres in point clouds available in literature.},
  isbn = {978-3-642-40245-6 978-3-642-40246-3},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\99TX74JX\Abuzaina 等 - 2013 - Sphere Detection in Kinect Point Clouds via the 3D Hough Transform.pdf}
}

@article{akritasApplicationsSingularvalueDecomposition2004,
  title = {Applications of Singular-Value Decomposition ({{SVD}})},
  author = {Akritas, Alkiviadis G. and Malaschonok, Gennadi I.},
  year = {2004},
  month = sep,
  journal = {Mathematics and Computers in Simulation},
  series = {Applications of {{Computer Algebra}} in {{Science}}, {{Engineering}}, {{Simulation}} and {{Special Software}}},
  volume = {67},
  number = {1},
  pages = {15--31},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2004.05.005},
  urldate = {2024-11-15},
  abstract = {Let A be an m{\texttimes}n matrix with m{$\geq$}n. Then one form of the singular-value decomposition of A is A=UT{$\Sigma$}V,where U and V are orthogonal and {$\Sigma$} is square diagonal. That is, UUT=Irank(A), VVT=Irank(A), U is rank(A){\texttimes}m, V is rank(A){\texttimes}n and {$\Sigma$}={$\sigma$}10{$\cdots$}000{$\sigma$}2{$\cdots$}00{$\vdots\vdots\ddots\vdots\vdots$}00{$\cdots\sigma$}rank(A)-1000{$\cdots$}0{$\sigma$}rank(A)is a rank(A){\texttimes}rank(A) diagonal matrix. In addition {$\sigma$}1{$\geq\sigma$}2{$\geq\cdots\geq\sigma$}rank(A){$>$}0. The {$\sigma$}i's are called the singular values of A and their number is equal to the rank of A. The ratio {$\sigma$}1/{$\sigma$}rank(A) can be regarded as a condition number of the matrix A. It is easily verified that the singular-value decomposition can be also written as A=UT{$\Sigma$}V={$\sum$}i=1rank(A){$\sigma$}iuiTvi.The matrix uiTvi is the outer product of the i-th row of U with the corresponding row of V. Note that each of these matrices can be stored using only m+n locations rather than mn locations. Using both forms presented above---and following Jerry Uhl's beautiful approach in the Calculus and Mathematica book series [Matrices, Geometry \& Mathematica, Math Everywhere Inc., 1999]---we show how SVD can be used as a tool for teaching Linear Algebra geometrically, and then apply it in solving least-squares problems and in data compression. In this paper we used the Computer Algebra system Mathematica to present a purely numerical problem. In general, the use of Computer Algebra systems has greatly influenced the teaching of mathematics, allowing students to concentrate on the main ideas and to visualize them.},
  langid = {american},
  keywords = {,Applications,Singular-value decompositions,svd},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\VZJ4WETQ\\Akritas和Malaschonok - 2004 - Applications of singular-value decomposition (SVD).pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LYFHZUA3\\S037847540400151X.html}
}

@article{al-sharadqahErrorAnalysisCircle2009,
  title = {Error Analysis for Circle Fitting Algorithms},
  author = {{Al-Sharadqah}, Ali and Chernov, Nikolai},
  year = {2009},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {3},
  number = {none},
  issn = {1935-7524},
  doi = {10.1214/09-EJS419},
  urldate = {2025-01-05},
  abstract = {We study the problem of fitting circles (or circular arcs) to data points observed with errors in both variables. A detailed error analysis for all popular circle fitting methods -- geometric fit, Ka{\r{}}sa fit, Pratt fit, and Taubin fit -- is presented. Our error analysis goes deeper than the traditional expansion to the leading order. We obtain higher order terms, which show exactly why and by how much circle fits differ from each other. Our analysis allows us to construct a new algebraic (non-iterative) circle fitting algorithm that outperforms all the existing methods, including the (previously regarded as unbeatable) geometric fit.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\W8H2GGW3\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.pdf}
}

@misc{ApplicationsSingularvalueDecomposition,
  title = {Applications of Singular-Value Decomposition ({{SVD}}) - {{ScienceDirect}}},
  urldate = {2024-11-15},
  howpublished = {https://www.sciencedirect.com/science/article/abs/pii/S037847540400151X},
  keywords = {,svd},
  file = {C:\Users\guaoxiang\Zotero\storage\WQ723ZBE\S037847540400151X.html}
}

@incollection{bederDirectSolutionsComputing2006,
  title = {Direct {{Solutions}} for {{Computing Cylinders}} from {{Minimal Sets}} of {{3D Points}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2006},
  author = {Beder, Christian and F{\"o}rstner, Wolfgang},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  volume = {3951},
  pages = {135--146},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11744023_11},
  urldate = {2025-01-04},
  abstract = {Efficient direct solutions for the determination of a cylinder from points are presented. The solutions range from the well known direct solution of a quadric to the minimal solution of a cylinder with five points. In contrast to the approach of G. Roth and M. D. Levine (1990), who used polynomial bases for representing the geometric entities, we use algebraic constraints on the quadric representing the cylinder. The solutions for six to eight points directly determine all the cylinder parameters in one step: (1) The eight-point-solution, similar to the estimation of the fundamental matrix, requires to solve for the roots of a 3rd-order-polynomial. (2) The seven-point-solution, similar to the sixpoint-solution for the relative orientation by J. Philip (1996), yields a linear equation system. (3) The six-point-solution, similar to the fivepoint-solution for the relative orientation by D. Nister (2003), yields a ten-by-ten eigenvalue problem. The new minimal five-point-solution first determines the direction and then the position and the radius of the cylinder. The search for the zeros of the resulting 6th order polynomials is efficiently realized using 2D-Bernstein polynomials. Also direct solutions for the special cases with the axes of the cylinder parallel to a coordinate plane or axis are given. The method is used to find cylinders in range data of an industrial site.},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\2S2HKMWH\Beder和Förstner - 2006 - Direct Solutions for Computing Cylinders from Minimal Sets of 3D Points.pdf}
}

@article{behley3DLiDARbasedSemantic2021,
  title = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences: {{The SemanticKITTI Dataset}}},
  shorttitle = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences},
  author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Gall, J{\"u}rgen and Stachniss, Cyrill},
  year = {2021},
  month = aug,
  journal = {The International Journal of Robotics Research},
  volume = {40},
  number = {8-9},
  pages = {959--967},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783649211006735},
  urldate = {2024-10-13},
  abstract = {A holistic semantic scene understanding exploiting all available sensor modalities is a core capability to master selfdriving in complex everyday traffic. To this end, we present the SemanticKITTI dataset that provides point-wise semantic annotations of Velodyne HDL-64E point clouds of the KITTI Odometry Benchmark. Together with the data, we also published three benchmark tasks for semantic scene understanding covering different aspects of semantic scene understanding: (1) semantic segmentation for point-wise classification using single or multiple point clouds as input, (2) semantic scene completion for predictive reasoning on the semantics and occluded regions, and (3) panoptic segmentation combining point-wise classification and assigning individual instance identities to separate objects of the same class. In this article, we provide details on our dataset showing an unprecedented number of fully annotated point cloud sequences, more information on our labeling process to efficiently annotate such a vast amount of point clouds, and lessons learned in this process. The dataset and resources are available at http://www.semantic-kitti.org.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\JPV7MRAC\Behley 等 - 2021 - Towards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences The SemanticKITTI D.pdf}
}

@misc{bermanMissionGPTMissionPlanner2024,
  title = {{{MissionGPT}}: {{Mission Planner}} for {{Mobile Robot}} Based on {{Robotics Transformer Model}}},
  shorttitle = {{{MissionGPT}}},
  author = {Berman, Vladimir and Bazhenov, Artem and Tsetserukou, Dzmitry},
  year = {2024},
  month = nov,
  number = {arXiv:2411.05107},
  eprint = {2411.05107},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.05107},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach to building mission planners based on neural networks with Transformer architecture and Large Language Models (LLMs). This approach demonstrates the possibility of setting a task for a mobile robot and its successful execution without the use of perception algorithms, based only on the data coming from the camera. In this work, a success rate of more than 50\% was obtained for one of the basic actions for mobile robots. The proposed approach is of practical importance in the field of warehouse logistics robots, as in the future it may allow to eliminate the use of markings, LiDARs, beacons and other tools for robot orientation in space. In conclusion, this approach can be scaled for any type of robot and for any number of robots.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\CWD9VXTW\\2411.05107v1-zh.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LJFK6KME\\Berman 等 - 2024 - MissionGPT Mission Planner for Mobile Robot based on Robotics Transformer Model.pdf}
}

@article{blackP0VisionLanguageActionFlow,
  title = {{$\Pi$}0: {{A Vision-Language-Action Flow Model}} for {{General Robot Control}}},
  author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and {Li-Bell}, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\GCDSK7RH\\Black 等 - π0 A Vision-Language-Action Flow Model for General Robot Control-zh.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\VGNCLN49\\Black 等 - π0 A Vision-Language-Action Flow Model for General Robot Control.pdf}
}

@misc{bochkovskiiDepthProSharp2024,
  title = {Depth {{Pro}}: {{Sharp Monocular Metric Depth}} in {{Less Than}} a {{Second}}},
  shorttitle = {Depth {{Pro}}},
  author = {Bochkovskii, Aleksei and Delaunoy, Ama{\"e}l and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  year = {2024},
  month = oct,
  number = {arXiv:2410.02073},
  eprint = {2410.02073},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02073},
  urldate = {2024-10-21},
  abstract = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  archiveprefix = {arXiv},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\KRYBNGSD\\Bochkovskii 等 - 2024 - Depth Pro Sharp Monocular Metric Depth in Less Than a Second.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\W2RSKNNQ\\2410.html}
}

@misc{brohanRT1RoboticsTransformer2023,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = aug,
  number = {arXiv:2212.06817},
  eprint = {2212.06817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.06817},
  urldate = {2024-12-03},
  abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\N4LZ8M2M\Brohan 等 - 2023 - RT-1 Robotics Transformer for Real-World Control at Scale.pdf}
}

@misc{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15818},
  eprint = {2307.15818},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.15818},
  urldate = {2024-12-03},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\2S75S6LR\Brohan 等 - 2023 - RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.pdf}
}

@misc{cheangGR2GenerativeVideoLanguageAction2024,
  title = {{{GR-2}}: {{A Generative Video-Language-Action Model}} with {{Web-Scale Knowledge}} for {{Robot Manipulation}}},
  shorttitle = {{{GR-2}}},
  author = {Cheang, Chi-Lam and Chen, Guangzeng and Jing, Ya and Kong, Tao and Li, Hang and Li, Yifeng and Liu, Yuxiao and Wu, Hongtao and Xu, Jiafeng and Yang, Yichu and Zhang, Hanbo and Zhu, Minzhao},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06158},
  eprint = {2410.06158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06158},
  urldate = {2024-12-03},
  abstract = {We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7\% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: https://gr2-manipulation.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8ME7SKHH\Cheang 等 - 2024 - GR-2 A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation.pdf}
}

@misc{chenUrdformerPipelineConstructing2024,
  title = {Urdformer: {{A Pipeline}} for {{Constructing Articulated Simulation Environments}} from {{Real-World Images}}},
  shorttitle = {Urdformer},
  author = {Chen, Zoey and Walsman, Aaron and Memmel, Marius and Mo, Kaichun and Fang, Alex and Vemuri, Karthikeya and Wu, Alan and Fox, Dieter and Gupta, Abhishek},
  year = {2024},
  month = may,
  number = {arXiv:2405.11656},
  eprint = {2405.11656},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand. A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\E8GI9TQA\Chen 等 - 2024 - URDFormer A Pipeline for Constructing Articulated Simulation Environments from Real-World Images.pdf}
}

@article{chernovLeastSquaresFitting2005,
  title = {Least {{Squares Fitting}} of {{Circles}}},
  author = {Chernov, N. and Lesort, C.},
  year = {2005},
  month = nov,
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {23},
  number = {3},
  pages = {239--252},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-005-0482-8},
  urldate = {2025-01-05},
  abstract = {Fitting standard shapes or curves to incomplete data (which represent only a small part of the curve) is a notoriously difficult problem. Even if the curve is quite simple, such as an ellipse or a circle, it is hard to reconstruct it from noisy data sampled along a short arc. Here we study the least squares fit (LSF) of circular arcs to incomplete scattered data. We analyze theoretical aspects of the problem and reveal the cause of unstable behavior of conventional algorithms. We also find a remedy that allows us to build another algorithm that accurately fits circles to data sampled along arbitrarily short arcs.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\DTSR4X6Q\Chernov和Lesort - 2005 - Least Squares Fitting of Circles.pdf}
}

@misc{chiDiffusionPolicyVisuomotor2024,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  year = {2024},
  month = mar,
  number = {arXiv:2303.04137},
  eprint = {2303.04137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  urldate = {2024-12-03},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\HBQLTKTM\2303.04137v5.pdf}
}

@misc{chiDiffusionPolicyVisuomotor2024a,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  year = {2024},
  month = mar,
  number = {arXiv:2303.04137},
  eprint = {2303.04137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  urldate = {2024-12-03},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AMUIUGTW\2303.04137v5.pdf}
}

@article{collinsInfinitesimalPlaneBasedPose2014,
  title = {{Infinitesimal Plane-Based Pose Estimation}},
  author = {Collins, Toby and Bartoli, Adrien},
  year = {2014},
  month = sep,
  journal = {International Journal of Computer Vision},
  volume = {109},
  number = {3},
  pages = {252--286},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-014-0725-5},
  urldate = {2024-11-27},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\K5H34TMH\Collins和Bartoli - 2014 - Infinitesimal Plane-Based Pose Estimation.pdf}
}

@article{coopeCircleFittingLinear1993,
  title = {Circle Fitting by Linear and Nonlinear Least Squares},
  author = {Coope, I. D.},
  year = {1993},
  month = feb,
  journal = {Journal of Optimization Theory and Applications},
  volume = {76},
  number = {2},
  pages = {381--388},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/BF00939613},
  urldate = {2025-01-05},
  abstract = {The problem of determining the circle of best fit to a set of points in the plane (or the obvious generalization to n-dimensions) is easily formulated as a nonlinear total least-squares problem which may be solved using a Gauss-Newton minimization algorithm. This straightforward approach is shown to be inefficient and extremely sensitive to the presence of outliers. An alternative formulation allows the problem to be reduced to a linear least squares problem which is trivially solved. The recommended approach is shown to have the added advantage of being much less sensitive to outliers than the nonlinear least squares approach.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\7V3ZLZRD\Coope - 1993 - Circle fitting by linear and nonlinear least squares.pdf}
}

@misc{cuiRecentAdvancesSpeech2024,
  title = {Recent {{Advances}} in {{Speech Language Models}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Speech Language Models}}},
  author = {Cui, Wenqian and Yu, Dianzhi and Jiao, Xiaoqi and Meng, Ziqiao and Zhang, Guangyan and Wang, Qichao and Guo, Yiwen and King, Irwin},
  year = {2024},
  month = oct,
  number = {arXiv:2410.03751},
  eprint = {2410.03751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03751},
  urldate = {2024-12-03},
  abstract = {Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)'', where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs)---end-toend models that generate speech without converting from text---have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize the evaluation metrics for SpeechLMs, and discuss the challenges and future research directions in this rapidly evolving field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\guaoxiang\Zotero\storage\69BXDFP8\Cui 等 - 2024 - Recent Advances in Speech Language Models A Survey.pdf}
}

@article{dalalNeuralMPGeneralist,
  title = {Neural {{MP}}: {{A Generalist Neural Motion Planner}}},
  author = {Dalal, Murtaza and Yang, Jiahui and Mendonca, Russell and Khaky, Youssef and Salakhutdinov, Ruslan and Pathak, Deepak},
  abstract = {The current paradigm for motion planning generates solutions from scratch for every new problem, which consumes significant amounts of time and computational resources. For complex, cluttered scenes, motion planning approaches can often take minutes to produce a solution, while humans are able to accurately and safely reach any goal in seconds by leveraging their prior experience. We seek to do the same by applying data-driven learning at scale to the problem of motion planning. Our approach builds a large number of complex scenes in simulation, collects expert data from a motion planner, then distills it into a reactive generalist policy. We then combine this with lightweight optimization to obtain a safe path for real world deployment. We perform a thorough evaluation of our method on 64 motion planning tasks across four diverse environments with randomized poses, scenes and obstacles, in the real world, demonstrating an improvement of 23\%, 17\% and 79\% motion planning success rate over state of the art sampling, optimization and learning based planning methods. Video results available at mihdalal.github.io/neuralmotionplanner.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\GE9N5LBM\Dalal 等 - Neural MP A Generalist Neural Motion Planner.pdf}
}

@article{davisonMonoSLAMRealTimeSingle2007,
  title = {{{MonoSLAM}}: {{Real-Time Single Camera SLAM}}},
  shorttitle = {{{MonoSLAM}}},
  author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {6},
  pages = {1052--1067},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2007.1049},
  urldate = {2024-11-06},
  abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the ``pure vision'' domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\88FCC8FE\Davison 等 - 2007 - MonoSLAM Real-Time Single Camera SLAM.pdf}
}

@misc{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03378},
  eprint = {2303.03378},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pretrained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AAEWE6UQ\Driess 等 - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf}
}

@article{du-mingtsaiBoundarybasedCornerDetection1997,
  title = {Boundary-Based Corner Detection Using Neural Networks},
  author = {{Du-Ming Tsai}},
  year = {1997},
  month = jan,
  journal = {Pattern Recognition},
  volume = {30},
  number = {1},
  pages = {85--97},
  issn = {00313203},
  doi = {10.1016/S0031-3203(96)00057-X},
  urldate = {2024-12-07},
  abstract = {In this paper we present a novel boundary-based corner detection approach using artificial neural networks (ANNs). Two neural networks are proposed: one for detecting corner points with high curvature, and the other for detecting tangent points and inflection points that generally have low curvature. For a given boundary point Pi, the first ANN uses the normalized coordinates of points on the forward ann (neighboring points succeeding Pi) or on the backward arm (neighboring points preceding Pi) of the point Pi as the input vector. The output feature of the network is the angle of the forward ann (or backward arm) with respect to the xaxis. The boundary point with sufficiently small angle between the forward and backward arms is identified as a corner. Since the feature points of tangency and inflection have relatively low curvature, the signs of curvature, rather than the magnitude of curvature, for points in the neighborhood of Pi are used as the input vector to the second ANN. The curvature sign at each boundary point is derived from the outputs of the first ANN. The outputs of the second ANN only respond to the sign patterns of tangent points and inflection points. By using both ANNs, all features of corners, tangent points and inflection points can be extracted from the boundary of any arbitrary shape. Experimental results have shown that the proposed ANNs have good detection and localization for objects in random orientations and with moderate scale changes. Copyright {\copyright} 1996. Published by Elsevier Science Ltd.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\P66L53KF\Du-Ming Tsai - 1997 - Boundary-based corner detection using neural networks.pdf}
}

@inproceedings{faberBuyersGuideEuclidean2001,
  title = {A {{Buyer}}'s {{Guide}} to {{Euclidean Elliptical Cylindrical}} and {{Conical Surface Fitting}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2001},
  author = {Faber, P and Fisher, R B},
  year = {2001},
  pages = {54.1-54.10},
  publisher = {British Machine Vision Association},
  address = {Manchester},
  doi = {10.5244/C.15.54},
  urldate = {2025-01-06},
  abstract = {The ability to construct CAD or other object models from edge and range data has a fundamental meaning in building a recognition and positioning system. While the problem of model fitting has been successfully addressed, the problem of efficient high accuracy and stability of the fitting is still an open problem. In the past researchers have used approximate distance functions rather than the real Euclidean distance because of computational efficiency. We now feel that machine speeds are sufficient to ask whether it is worth considering Euclidean fitting again. This paper address the problem of estimation of elliptical cylinder and cone surfaces to 3D data by a constrained Euclidean fitting. We study and compare the performance of various distance functions in terms of correctness, robustness and pose invariance, and present our results improving known fitting methods by closed form expressions of the real Euclidean distance.},
  isbn = {978-1-901725-16-2},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\UMBX5M7V\Faber和Fisher - 2001 - A Buyer's Guide to Euclidean Elliptical Cylindrical and Conical Surface Fitting.pdf}
}

@article{feiNewShortarcFitting2020,
  title = {A New Short-Arc Fitting Method with High Precision Using {{Adam}} Optimization Algorithm},
  author = {Fei, Zhigen and Wu, Zhiying and Xiao, Yanqiu and Ma, Jun and He, Wenbin},
  year = {2020},
  month = jun,
  journal = {Optik},
  volume = {212},
  pages = {164788},
  issn = {0030-4026},
  doi = {10.1016/j.ijleo.2020.164788},
  urldate = {2024-11-20},
  abstract = {The high-precision short-arc measurement is a huge challenge in scientific research and engineering practice. The popular traditional least square fitting (TLSF) however fails to achieve the precise fitting parameters when the arc gets shorter. In this work, an inequation constrained fitting (ICF) method based on four-parameter circle equation is presented. Lagrangian multiplier and Karush-Kuhn-Tucker criteria are used to redefine the objective function. After that, Adam algorithm is utilized to solve the objective function in iterative way. Adam algorithm has a strong ability to resist noise pollution by virtue of modifying continually the first-order momentum and second-order momentum with average of gradients during the course of iteration. Finally, simulation and experimental results show that our ICF method is more robust and high-precision than TLSF and Hyper method, so it is very competent to measure short arcs with noise even their central angles are close to 5{$^\circ$}.},
  langid = {american},
  keywords = {,Adam algorithm,Lagrangian multiplier,Short-arc fitting},
  file = {C:\Users\guaoxiang\Zotero\storage\36ADMCMA\S0030402620306240.html}
}

@article{feiNewShortarcFitting2020a,
  title = {A New Short-Arc Fitting Method with High Precision Using {{Adam}} Optimization Algorithm},
  author = {Fei, Zhigen and Wu, Zhiying and Xiao, Yanqiu and Ma, Jun and He, Wenbin},
  year = {2020},
  month = jun,
  journal = {Optik},
  volume = {212},
  pages = {164788},
  issn = {00304026},
  doi = {10.1016/j.ijleo.2020.164788},
  urldate = {2024-11-20},
  abstract = {The high-precision short-arc measurement is a huge challenge in scientific research and engineering practice. The popular traditional least square fitting (TLSF) however fails to achieve the precise fitting parameters when the arc gets shorter. In this work, an inequation constrained fitting (ICF) method based on four-parameter circle equation is presented. Lagrangian multiplier and Karush-Kuhn-Tucker criteria are used to redefine the objective function. After that, Adam algorithm is utilized to solve the objective function in iterative way. Adam algorithm has a strong ability to resist noise pollution by virtue of modifying continually the first-order momentum and second-order momentum with average of gradients during the course of iteration. Finally, simulation and experimental results show that our ICF method is more robust and high-precision than TLSF and Hyper method, so it is very competent to measure short arcs with noise even their central angles are close to 5{$^\circ$}.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ZFA4JE24\Fei 等 - 2020 - A new short-arc fitting method with high precision using Adam optimization algorithm.pdf}
}

@misc{firooziFoundationModelsRobotics2023,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6EADR4YW\\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\C7KSUGVZ\\2312.html}
}

@misc{firooziFoundationModelsRobotics2023a,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper1 can be found here.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\97V5HWLE\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf}
}

@article{fitzgibbonDirectLeastSquare1999,
  title = {{Direct least square fitting of ellipses}},
  author = {Fitzgibbon, A. and Pilu, M. and Fisher, R.B.},
  year = {1999},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {5},
  pages = {476--480},
  issn = {01628828},
  doi = {10.1109/34.765658},
  urldate = {2024-11-27},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\LUJ4TW3Z\Fitzgibbon 等 - 1999 - Direct least square fitting of ellipses.pdf}
}

@misc{fuMobileALOHALearning2024,
  title = {Mobile {{ALOHA}}: {{Learning Bimanual Mobile Manipulation}} with {{Low-Cost Whole-Body Teleoperation}}},
  shorttitle = {Mobile {{ALOHA}}},
  author = {Fu, Zipeng and Zhao, Tony Z. and Finn, Chelsea},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02117},
  eprint = {2401.02117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.02117},
  urldate = {2024-12-03},
  abstract = {Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system [104] with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90\%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\guaoxiang\Zotero\storage\MX6MQSNW\Fu 等 - 2024 - Mobile ALOHA Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation.pdf}
}

@misc{garciaGeneralizableVisionLanguageRobotic2024,
  title = {Towards {{Generalizable Vision-Language Robotic Manipulation}}: {{A Benchmark}} and {{LLM-guided 3D Policy}}},
  shorttitle = {Towards {{Generalizable Vision-Language Robotic Manipulation}}},
  author = {Garcia, Ricardo and Chen, Shizhe and Schmid, Cordelia},
  year = {2024},
  month = oct,
  number = {arXiv:2410.01345},
  eprint = {2410.01345},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Generalizing language-conditioned robotic policies to new tasks remains a significant challenge, hampered by the lack of suitable simulation benchmarks. In this paper, we address this gap by introducing GemBench, a novel benchmark to assess generalization capabilities of vision-language robotic manipulation policies. GemBench incorporates seven general action primitives and four levels of generalization, spanning novel placements, rigid and articulated objects, and complex long-horizon tasks. We evaluate state-of-the-art approaches on GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich 3D information for action prediction conditioned on language. While 3D-LOTUS excels in both efficiency and performance on seen tasks, it struggles with novel tasks. To address this, we present 3D-LOTUS++, a framework that integrates 3D-LOTUS's motion planning capabilities with the task planning capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++ achieves state-ofthe-art performance on novel tasks of GemBench, setting a new standard for generalization in robotic manipulation. The benchmark, codes and trained models are available at https: //www.di.ens.fr/willow/research/gembench/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\USPYPEPA\Garcia 等 - 2024 - Towards Generalizable Vision-Language Robotic Manipulation A Benchmark and LLM-guided 3D Policy.pdf}
}

@article{gorokhovatskyiSearchVisualObjects2023,
  title = {Search for {{Visual Objects}} by {{Request}} in the {{Form}} of a {{Cluster Representation}} for the {{Structural Image Description}}},
  author = {Gorokhovatskyi, Volodymyr and Tvoroshenko, Iryna and Kobylin, Oleg and Vlasenko, Nataliia},
  year = {2023},
  month = may,
  journal = {Advances in Electrical and Electronic Engineering},
  volume = {21},
  number = {1},
  pages = {19--27},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v21i1.4661},
  urldate = {2024-11-07},
  abstract = {The key task of computer vision is the recognition of visual objects in the analysed image. This paper proposes a method of searching for objects in an image, based on the identification of a cluster representation of the query descriptions and the current image of the window with the calculation of the relevance measure. The implementation of a cluster representation significantly increases the speed of identification or classification of visual objects while maintaining a sufficient level of accuracy. Based on the development of models for the analysis and processing of a set of descriptors of keypoints, we have obtained an effective method for the identification of visual objects. A comparative experiment with the traditional method has been conducted, where a linear search for the nearest descriptor was implemented for identification without using a cluster representation of the description. In the experiment, a speed gain for the developed method has been obtained in comparison with the traditional one by approximately 5.2 times with the same level of accuracy. The method can be used in applied tasks where the time of object identification is critical. The developed method can be applied to search for several objects of different classes. The effectiveness of the method can be increased by varying the values of its parameters and adapting to the characteristics of the data.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\NJAIKU54\Gorokhovatskyi 等 - 2023 - Search for Visual Objects by Request in the Form of a Cluster Representation for the Structural Imag.pdf}
}

@article{halirVNUMERICALLYSTABLEDIRECT,
  title = {{{NUMERICALLY STABLE DIRECT LEAST SQUARES FITTING OF ELLIPSES}}},
  author = {Hal{\i}r{\textasciicaron}, Radim and Flusser, Jan},
  abstract = {This paper presents a numerically stable non-iterative algorithm for fitting an ellipse to a set of data points. The approach is based on a least squares minimization and it guarantees an ellipse-specific solution even for scattered or noisy data. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method which can be easily implemented. The proposed algorithm has no computational ambiguity and it is able to fit more than 100,000 points in a second.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\X9V7S65S\\translated_WSCG98.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\YA8KESZU\\Halırˇ和Flusser - NUMERICALLY STABLE DIRECT LEAST SQUARES FITTING OF ELLIPSES.pdf}
}

@misc{hannunDeepSpeechScaling2014,
  title = {Deep {{Speech}}: {{Scaling}} up End-to-End Speech Recognition},
  shorttitle = {Deep {{Speech}}},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  year = {2014},
  month = dec,
  number = {arXiv:1412.5567},
  eprint = {1412.5567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.5567},
  urldate = {2024-12-03},
  abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a ``phoneme.'' Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\guaoxiang\Zotero\storage\AQS7RZGY\Hannun 等 - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf}
}

@misc{huangInstruct2ActMappingMultimodality2023,
  title = {{{Instruct2Act}}: {{Mapping Multi-modality Instructions}} to {{Robotic Actions}} with {{Large Language Model}}},
  shorttitle = {{{Instruct2Act}}},
  author = {Huang, Siyuan and Jiang, Zhengkai and Dong, Hao and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  year = {2023},
  month = may,
  number = {arXiv:2305.11176},
  eprint = {2305.11176},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11176},
  urldate = {2024-12-03},
  abstract = {Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\GZ4YRD76\Huang 等 - 2023 - Instruct2Act Mapping Multi-modality Instructions to Robotic Actions with Large Language Model.pdf}
}

@misc{huangManipVQAInjectingRobotic2024,
  title = {{{ManipVQA}}: {{Injecting Robotic Affordance}} and {{Physically Grounded Information}} into {{Multi-Modal Large Language Models}}},
  shorttitle = {{{ManipVQA}}},
  author = {Huang, Siyuan and Ponomarenko, Iaroslav and Jiang, Zhengkai and Li, Xiaoqi and Hu, Xiaobin and Gao, Peng and Li, Hongsheng and Dong, Hao},
  year = {2024},
  month = aug,
  number = {arXiv:2403.11289},
  eprint = {2403.11289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.11289},
  urldate = {2024-12-03},
  abstract = {While the integration of Multi-modal Large Language Models (MLLMs) with robotic systems has significantly improved robots' ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic imagetext pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a unified VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@article{huangNovelAlgorithmFitting2021,
  title = {A Novel Algorithm: Fitting a Spatial Arc to Noisy Point Clouds with High Accuracy and Reproducibility},
  shorttitle = {A Novel Algorithm},
  author = {Huang, Shuai and Chen, Ming and Lu, ShengLian and Chen, ShouXin and Zha, YongJian},
  year = {2021},
  month = aug,
  journal = {Measurement Science and Technology},
  volume = {32},
  number = {8},
  pages = {085004},
  issn = {0957-0233, 1361-6501},
  doi = {10.1088/1361-6501/abf867},
  urldate = {2024-11-20},
  abstract = {Fitting a spatial arc to noisy point clouds with high accuracy and reproducibility is challenging, although it is important in many applications, such as precise measurement, computerized numerical control machining and robotic path planning. In optical measuring applications, an arc-shaped object is usually first scanned as point clouds by a 3D camera or multiple charge-coupled device cameras, and arc fitting is then invoked to fit these point clouds, obtaining the measuring radius and center. The accuracy of the arc-fitting algorithm plays an important role in the arc-measuring precision. In this paper, a novel algorithm is proposed to fit a spatial arc of high accuracy and reproducibility to noisy point clouds. This method combines the repeated least trimmed squares idea with the smoothing fairness function, i.e. discrete curvature, to devise the objective function, which is solved iteratively. This algorithm can successfully filter noise and fit a highly accurate arc to noisy point clouds with high reproducibility. Seven popular arc-fitting algorithms are implemented as benchmarks and both simulated and real data scanned from physical objects are tested to validate that the proposed algorithm performs best. The proposed algorithm is efficient and can be easily implemented in industrial applications.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\H2GGVS86\Huang 等 - 2021 - A novel algorithm fitting a spatial arc to noisy point clouds with high accuracy and reproducibilit.pdf}
}

@misc{huangReKepSpatioTemporalReasoning2024,
  title = {{{ReKep}}: {{Spatio-Temporal Reasoning}} of {{Relational Keypoint Constraints}} for {{Robotic Manipulation}}},
  shorttitle = {{{ReKep}}},
  author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and {Fei-Fei}, Li},
  year = {2024},
  month = nov,
  number = {arXiv:2409.01652},
  eprint = {2409.01652},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.01652},
  urldate = {2024-12-03},
  abstract = {Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\4WG6ERKS\Huang 等 - 2024 - ReKep Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation.pdf}
}

@misc{huangVoxPoserComposable3D2023,
  title = {{{VoxPoser}}: {{Composable 3D Value Maps}} for {{Robotic Manipulation}} with {{Language Models}}},
  shorttitle = {{{VoxPoser}}},
  author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and {Fei-Fei}, Li},
  year = {2023},
  month = nov,
  number = {arXiv:2307.05973},
  eprint = {2307.05973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a largescale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at voxposer.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\CANL9GNR\Huang 等 - 2023 - VoxPoser Composable 3D Value Maps for Robotic Manipulation with Language Models.pdf}
}

@misc{huaREDEEndendObject2021,
  title = {{{REDE}}: {{End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}}},
  shorttitle = {{{REDE}}},
  author = {Hua, Weitong and Zhou, Zhongxiang and Wu, Jun and Huang, Huang and Wang, Yue and Xiong, Rong},
  year = {2021},
  month = feb,
  eprint = {2010.12807},
  primaryclass = {cs},
  doi = {10.1109/LRA.2021.3062304},
  urldate = {2024-11-01},
  abstract = {Object 6D pose estimation is a fundamental task in many applications. Conventional methods solve the task by detecting and matching the keypoints, then estimating the pose. Recent efforts bringing deep learning into the problem mainly overcome the vulnerability of conventional methods to environmental variation due to the hand-crafted feature design. However, these methods cannot achieve end-to-end learning and good interpretability at the same time. In this paper, we propose REDE, a novel end-to-end object pose estimator using RGB-D data, which utilizes network for keypoint regression, and a differentiable geometric pose estimator for pose error back-propagation. Besides, to achieve better robustness when outlier keypoint prediction occurs, we further propose a differentiable outliers elimination method that regresses the candidate result and the confidence simultaneously. Via confidence weighted aggregation of multiple candidates, we can reduce the effect from the outliers in the final estimation. Finally, following the conventional method, we apply a learnable refinement process to further improve the estimation. The experimental results on three benchmark datasets show that REDE slightly outperforms the state-of-the-art approaches and is more robust to object occlusion. Our code is available at https://github.com/HuaWeitong/REDE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\CQTLNTZQ\Hua 等 - 2021 - REDE End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination.pdf}
}

@article{hubertDeterministicAlgorithmRobust,
  title = {A Deterministic Algorithm for Robust Location and Scatter},
  author = {Hubert, Mia},
  abstract = {Most algorithms for highly robust estimators of multivariate location and scatter start by drawing a large number of random subsets. For instance, the FASTMCD algorithm of Rousseeuw and Van Driessen (1999) starts in this way, and then takes so-called concentration steps to obtain a more accurate approximation to the MCD. The FASTMCD algorithm is affine equivariant but not permutation invariant. In this article we present a deterministic algorithm, denoted as DetMCD, which does not use random subsets and is even faster. It computes a small number of deterministic initial estimators, followed by concentration steps. DetMCD is permutation invariant and very close to affine equivariant. We compare it to FASTMCD and to the OGK estimator of Maronna and Zamar (2002). We also illustrate it on real and simulated data sets, with applications involving principal component analysis, classification and time series analysis. Supplemental material (Matlab code of the DetMCD algorithm and the data sets) is available online.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\ADKAWM6P\\Hubert - A deterministic algorithm for robust location and scatter.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\ZKYI7RXH\\translated_A-Deterministic-Algorithm-for-Robust-Location-and-Scatter.pdf}
}

@article{hubertMinimumCovarianceDeterminant2018,
  title = {Minimum {{Covariance Determinant}} and {{Extensions}}},
  author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
  year = {2018},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {10},
  number = {3},
  eprint = {1709.07045},
  primaryclass = {stat},
  pages = {e1421},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1421},
  urldate = {2024-12-27},
  abstract = {The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\guaoxiang\Zotero\storage\LV4ZA43Y\Hubert 等 - 2018 - Minimum Covariance Determinant and Extensions.pdf}
}

@article{hubertROBPCANewApproach2005,
  title = {{{ROBPCA}}: {{A New Approach}} to {{Robust Principal Component Analysis}}},
  shorttitle = {{{ROBPCA}}},
  author = {Hubert, Mia and Rousseeuw, Peter J and Vanden Branden, Karlien},
  year = {2005},
  month = feb,
  journal = {Technometrics},
  volume = {47},
  number = {1},
  pages = {64--79},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017004000000563},
  urldate = {2024-12-27},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\NMHUFRWH\Hubert 等 - 2005 - ROBPCA A New Approach to Robust Principal Component Analysis.pdf}
}

@article{hubertROBPCANewApproach2005a,
  title = {{{ROBPCA}}: {{A New Approach}} to {{Robust Principal Component Analysis}}},
  shorttitle = {{{ROBPCA}}},
  author = {Hubert, Mia and Rousseeuw, Peter J and Vanden Branden, Karlien},
  year = {2005},
  month = feb,
  journal = {Technometrics},
  volume = {47},
  number = {1},
  pages = {64--79},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017004000000563},
  urldate = {2025-01-15},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\LCJIAP3Q\Hubert 等 - 2005 - ROBPCA A New Approach to Robust Principal Component Analysis.pdf}
}

@article{jauhriRobotLearningMobile2022,
  title = {Robot {{Learning}} of {{Mobile Manipulation}} with {{Reachability Behavior Priors}}},
  author = {Jauhri, Snehal and Peters, Jan and Chalvatzaki, Georgia},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  eprint = {2203.04051},
  primaryclass = {cs},
  pages = {8399--8406},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3188109},
  urldate = {2024-09-25},
  abstract = {Mobile Manipulation (MM) systems are ideal candidates for taking up the role of personal assistants in unstructured real-world environments. MM requires effective coordination of the robot's embodiments for tasks that require both mobility and manipulation. Reinforcement Learning (RL) holds the promise of endowing robots with adaptive behaviors, but most methods require large amounts of data. In this work, we study the integration of robotic reachability priors in actor-critic RL methods for accelerating the learning of MM for reaching and fetching tasks. Namely, we consider the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target. We devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization. Next, we train a reachability prior using data from the operational robot workspace, inspired by classical methods. Subsequently, we derive Boosted HyRL (BHyRL), a novel actor-critic algorithm that benefits from modeling Q-functions as a sum of residuals. For every new task, we transfer our learned residuals and learn the component of the Q-function that is task-specific, hence, maintaining the task structure from prior behaviors. Moreover, we find that regularizing the target policy with a prior policy yields more expressive behaviors. We evaluate our method in simulation in reaching and fetching tasks of increasing difficulty, and show the superior performance of BHyRL against baseline methods. Finally, we zero-transfer our learned 6D fetching policy with BHyRL to our MM robot: TIAGo++. For more details, refer to our project site: https://irosalab.com/rlmmbp.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {TLDR: This work considers the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target, and devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization.},
  file = {C:\Users\guaoxiang\Zotero\storage\3FUWWX5B\Jauhri 等 - 2022 - Robot Learning of Mobile Manipulation with Reachability Behavior Priors.pdf}
}

@misc{jiangRTMPoseRealTimeMultiPerson2023,
  title = {{{RTMPose}}: {{Real-Time Multi-Person Pose Estimation}} Based on {{MMPose}}},
  shorttitle = {{{RTMPose}}},
  author = {Jiang, Tao and Lu, Peng and Zhang, Li and Ma, Ningsheng and Han, Rui and Lyu, Chengqi and Li, Yining and Chen, Kai},
  year = {2023},
  month = jul,
  number = {arXiv:2303.07399},
  eprint = {2303.07399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. To bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multiperson pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75.8\% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-x achieves 65.3\% AP on COCO-WholeBody. To further evaluate RTMPose's capability in critical real-time applications, we also report the performance after deploying on the mobile device. Our RTMPose-s model achieves 72.2\% AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing open-source libraries. Our code and models are available at https://github.com/openmmlab/mmpose/tree/main/projects/rtmpose.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\LPV75RBD\Jiang 等 - 2023 - RTMPose Real-Time Multi-Person Pose Estimation based on MMPose.pdf}
}

@misc{jiangVIMAGeneralRobot2023,
  title = {{{VIMA}}: {{General Robot Manipulation}} with {{Multimodal Prompts}}},
  shorttitle = {{{VIMA}}},
  author = {Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and {Fei-Fei}, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  year = {2023},
  month = may,
  number = {arXiv:2210.03094},
  eprint = {2210.03094},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. This work shows that we can express a wide spectrum of robot manipulation tasks with multimodal prompts, interleaving textual and visual tokens. We design a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. To train and evaluate VIMA, we develop a new simulation benchmark with thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and four levels of evaluation protocol for systematic generalization. VIMA achieves strong scalability in both model capacity and data size. It outperforms prior SOTA methods in the hardest zero-shot generalization setting by up to 2.9{\texttimes} task success rate given the same training data. With 10{\texttimes} less training data, VIMA still performs 2.7{\texttimes} better than the top competing approach. We open-source all code, pretrained models, dataset, and simulation benchmark at https://vimalabs.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\293Q63E9\Jiang 等 - 2023 - VIMA General Robot Manipulation with Multimodal Prompts.pdf}
}

@inproceedings{kalashnikovScalableDeepReinforcement2018,
  title = {Scalable {{Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Robot Learning}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  year = {2018},
  month = oct,
  pages = {651--673},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-10-13},
  abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\C5D82LVY\Kalashnikov 等 - 2018 - Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.pdf}
}

@misc{kimOpenVLAOpenSourceVisionLanguageAction2024,
  title = {{{OpenVLA}}: {{An Open-Source Vision-Language-Action Model}}},
  shorttitle = {{{OpenVLA}}},
  author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  year = {2024},
  month = sep,
  number = {arXiv:2406.09246},
  eprint = {2406.09246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.09246},
  urldate = {2024-12-03},
  abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8ENWJAT2\Kim 等 - 2024 - OpenVLA An Open-Source Vision-Language-Action Model.pdf}
}

@article{lalondeAutomaticThreeDimensionalPoint,
  title = {Automatic {{Three-Dimensional Point Cloud Processing}} for {{Forest Inventory}}},
  author = {Lalonde, Jean-Francis and Vandapel, Nicolas and Hebert, Martial},
  abstract = {In this paper, we propose an approach that enables automatic, fast and accurate tree trunks segmentation from three-dimensional (3-D) laser data. Results have been demonstrated in real-time on-board a ground mobile robot. In addition, we propose an approach to estimate tree diameter at breast height (dbh) that was tested off-line on a variety of ground laser scanner data. Results are also presented for detection of tree trunks in aerial laser data. The underlying techniques using in all cases rely on 3-D geometry analysis of point clouds and geometric primitives fitting.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\JKLWHUQH\\translated_file.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\VXAXYL6N\\Lalonde 等 - Automatic Three-Dimensional Point Cloud Processing for Forest Inventory.pdf}
}

@misc{leyderRobPyPythonPackage2024,
  title = {{{RobPy}}: A {{Python Package}} for {{Robust Statistical Methods}}},
  shorttitle = {{{RobPy}}},
  author = {Leyder, Sarah and Raymaekers, Jakob and Rousseeuw, Peter J. and Servotte, Thomas and Verdonck, Tim},
  year = {2024},
  month = nov,
  number = {arXiv:2411.01954},
  eprint = {2411.01954},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.01954},
  urldate = {2025-01-06},
  abstract = {Robust estimation provides essential tools for analyzing data that contain outliers, ensuring that statistical models remain reliable even in the presence of some anomalous data. While robust methods have long been available in R, Python users have lacked a comprehensive package that offers these methods in a cohesive framework. RobPy addresses this gap by offering a wide range of robust methods in Python, built upon established libraries including NumPy, SciPy, and scikit-learn. This package includes tools for robust preprocessing, univariate estimation, covariance matrices, regression, and principal component analysis, which are able to detect outliers and to mitigate their effect. In addition, RobPy provides specialized diagnostic plots for visualizing outliers in both casewise and cellwise contexts. This paper presents the structure of the RobPy package, demonstrates its functionality through examples, and compares its features to existing implementations in other statistical software. By bringing robust methods to Python, RobPy enables more users to perform robust data analysis in a modern and versatile programming language.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\E7TXWDAI\Leyder 等 - 2024 - RobPy a Python Package for Robust Statistical Methods.pdf}
}

@misc{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2023},
  month = may,
  number = {arXiv:2209.07753},
  eprint = {2209.07753},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\EMNPHLHU\Liang 等 - 2023 - Code as Policies Language Model Programs for Embodied Control.pdf}
}

@misc{liManipLLMEmbodiedMultimodal2023,
  title = {{{ManipLLM}}: {{Embodied Multimodal Large Language Model}} for {{Object-Centric Robotic Manipulation}}},
  shorttitle = {{{ManipLLM}}},
  author = {Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.16217},
  eprint = {2312.16217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\D2Y4PPCK\Li 等 - 2023 - ManipLLM Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation.pdf}
}

@article{limbergerRealTimeDetectionPlanar,
  title = {Real-{{Time Detection}} of {{Planar Regions}} in {{Unorganized Point Clouds}}},
  author = {Limberger, Frederico Artur},
  abstract = {Automatic detection of planar regions in point clouds is an important step for many graphics, image processing, and computer vision applications. While laser scanners and digital photography have allowed us to capture increasingly larger datasets, previous techniques are computationally expensive, being unable to achieve real-time performance for datasets containing tens of thousands of points, even when detection is performed in a non-deterministic way. We present a deterministic technique for plane detection in unorganized point clouds whose cost is O(n log n) in the number of input samples. It is based on an efficient Hough-transform voting scheme and works by clustering approximately co-planar points and by casting votes for these clusters on a spherical accumulator using a trivariate Gaussian kernel. A comparison with competing techniques shows that our approach is considerably faster and scales significantly better than previous ones, being the first practical solution for deterministic plane detection in large unorganized point clouds.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\LMB42MZB\Limberger - Real-Time Detection of Planar Regions in Unorganized Point Clouds.pdf}
}

@misc{liMMRoAreMultimodal2024,
  title = {{{MMRo}}: {{Are Multimodal LLMs Eligible}} as the {{Brain}} for {{In-Home Robotics}}?},
  shorttitle = {{{MMRo}}},
  author = {Li, Jinming and Zhu, Yichen and Xu, Zhiyuan and Gu, Jindong and Zhu, Minjie and Liu, Xin and Liu, Ning and Peng, Yaxin and Feng, Feifei and Tang, Jian},
  year = {2024},
  month = jun,
  number = {arXiv:2406.19693},
  eprint = {2406.19693},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {It is fundamentally challenging for robots to serve as useful assistants in human environments because this requires addressing a spectrum of sub-problems across robotics, including perception, language understanding, reasoning, and planning. The recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated their exceptional abilities in solving complex mathematical problems, mastering commonsense and abstract reasoning. This has led to the recent utilization of MLLMs as the "brain" in robotic systems, enabling these models to conduct high-level planning prior to triggering low-level control actions for task execution. However, it remains uncertain whether existing MLLMs are reliable in serving the brain role of robots. In this study, we introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo) benchmark, which tests the capability of MLLMs for robot applications. Specifically, we identify four essential capabilities --- perception, task planning, visual reasoning, and safety measurement --- that MLLMs must possess to qualify as the robot's central processing unit. We have developed several scenarios for each capability, resulting in a total of 14 metrics for evaluation. We present experimental results for various MLLMs, including both commercial and open-source models, to assess the performance of existing systems. Our findings indicate that no single model excels in all areas, suggesting that current MLLMs are not yet trustworthy enough to serve as the cognitive core for robots. Our data can be found in https://mm-robobench.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  annotation = {titleTranslation: MMRo：多模态大语言模型是否具备成为家庭机器人大脑的资格？},
  file = {C:\Users\guaoxiang\Zotero\storage\9PXHGIDT\Li 等 - 2024 - MMRo Are Multimodal LLMs Eligible as the Brain for In-Home Robotics.pdf}
}

@article{linLeastSquaresAlgorithm2017,
  title = {A Least Squares Algorithm for Fitting Data Points to a Circular Arc Cam},
  author = {Lin, Shan and Jusko, Otto and H{\"a}rtig, Frank and Seewig, J{\"o}rg},
  year = {2017},
  month = may,
  journal = {Measurement},
  volume = {102},
  pages = {170--178},
  issn = {02632241},
  doi = {10.1016/j.measurement.2017.01.059},
  urldate = {2024-11-20},
  abstract = {Precise evaluation of form error is important for quality control in the manufacture of camshafts. For circular arc cams, a conventional method is to fit each arc segment of the cam individually. In such a case, at the connecting points of two fitted segments, there may be discontinuity or non-smoothness. In this paper, a global cam fitting algorithm based on the nonlinear least squares method is proposed. A circular arc cam is represented by the mathematic function in terms of form, rotation and position parameters. By imposing parameter constraints, a closed and smooth profile can be obtained as the result of fitting. In order to evaluate the performance of the proposed algorithm, the uncertainties of the fitted parameters are estimated by the GUM uncertainty framework and Monte Carlo simulations. Compared to the conventional cam fit, the uncertainties obtained by the proposed algorithm are lower. Additionally, the factors which significantly affect the fitting results are specified.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\DWQ9HATG\Lin 等 - 2017 - A least squares algorithm for fitting data points to a circular arc cam.pdf}
}

@article{linPlanarBasedAdaptiveSampling2016,
  title = {Planar-{{Based Adaptive Down-Sampling}} of {{Point Clouds}}},
  author = {Lin, Yun-Jou and Benziger, Ronald R and Habib, Ayman},
  year = {2016},
  month = dec,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume = {82},
  number = {12},
  pages = {955--966},
  issn = {00991112},
  doi = {10.14358/PERS.82.12.955},
  urldate = {2024-12-05},
  abstract = {Derived point clouds from laser scanners and image-based dense-matching techniques usually include tremendous number of points. Processing (e.g., segmenting) such huge dataset is time-consuming and might not be necessary. For example, a planar surface just needs few points to be defined. In contrast, linear/cylindrical and rough features require more points for reliable modeling since during the data acquisition process, only a portion of linear/cylindrical features is present in the point cloud. This paper introduces an adaptive down-sampling strategy for removing redundant points from high density planar regions while retaining points in planar areas with sparse points and all the points within linear/cylindrical and rough neighborhoods. To demonstrate the feasibility and performance of the proposed procedure, a comparison of segmentation results using original laser and image-based point clouds as well as the adaptively, uniformly, and point-spacing-based down-sampled point clouds are presented while commenting on the computational efficiency and the segmentation quality.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ETHHNUIK\Lin 等 - 2016 - Planar-Based Adaptive Down-Sampling of Point Clouds.pdf}
}

@article{liuAlgorithmResearchBased2024,
  title = {{Algorithm Research Based on an Elliptical Arc Fitting Curve}},
  author = {Liu, Qingjian and Li, Pei and Huang, Gangpeng and Zhang, Xu and Liu, Shuo and Yang, Ziyi and Hao, Tianze},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {113113--113125},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3416465},
  urldate = {2024-11-28},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\QFVNMQRN\Liu 等 - 2024 - Algorithm Research Based on an Elliptical Arc Fitting Curve.pdf}
}

@misc{liuAligningCyberSpace2024,
  title = {Aligning {{Cyber Space}} with {{Physical World}}: {{A Comprehensive Survey}} on {{Embodied AI}}},
  shorttitle = {Aligning {{Cyber Space}} with {{Physical World}}},
  author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  year = {2024},
  month = aug,
  number = {arXiv:2407.06886},
  eprint = {2407.06886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06886},
  urldate = {2024-12-02},
  abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github. com/HCPLab-SYSU/Embodied AI Paper List.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\QKFM78VQ\Liu 等 - 2024 - Aligning Cyber Space with Physical World A Comprehensive Survey on Embodied AI.pdf}
}

@misc{liuAligningCyberSpace2024a,
  title = {Aligning {{Cyber Space}} with {{Physical World}}: {{A Comprehensive Survey}} on {{Embodied AI}}},
  shorttitle = {Aligning {{Cyber Space}} with {{Physical World}}},
  author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  year = {2024},
  month = aug,
  number = {arXiv:2407.06886},
  eprint = {2407.06886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06886},
  urldate = {2024-12-02},
  abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github. com/HCPLab-SYSU/Embodied AI Paper List.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\4W3ET2UM\Liu 等 - 2024 - Aligning Cyber Space with Physical World A Comprehensive Survey on Embodied AI.pdf}
}

@misc{liuLLMBasedHumanRobotCollaboration2023,
  title = {{{LLM-Based Human-Robot Collaboration Framework}} for {{Manipulation Tasks}}},
  author = {Liu, Haokun and Zhu, Yaonan and Kato, Kenji and Kondo, Izumi and Aoyama, Tadayoshi and Hasegawa, Yasuhisa},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14972},
  eprint = {2308.14972},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14972},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach to enhance autonomous robotic manipulation using the Large Language Model (LLM) for logical inference, converting high-level language commands into sequences of executable motion functions. The proposed system combines the advantage of LLM with YOLO-based environmental perception to enable robots to autonomously make reasonable decisions and task planning based on the given commands. Additionally, to address the potential inaccuracies or illogical actions arising from LLM, a combination of teleoperation and Dynamic Movement Primitives (DMP) is employed for action correction. This integration aims to improve the practicality and generalizability of the LLM-based human-robot collaboration system.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\BX5SMRQ6\\Liu 等 - 2023 - LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\EDJGHERZ\\2308.14972v1-zh.pdf}
}

@misc{liUniDoorManipLearningUniversal2024,
  title = {{{UniDoorManip}}: {{Learning Universal Door Manipulation Policy Over Large-scale}} and {{Diverse Door Manipulation Environments}}},
  shorttitle = {{{UniDoorManip}}},
  author = {Li, Yu and Zhang, Xiaojie and Wu, Ruihai and Zhang, Zilong and Geng, Yiran and Dong, Hao and He, Zhaofeng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.02604},
  eprint = {2403.02604},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.02604},
  urldate = {2024-12-03},
  abstract = {Learning a universal manipulation policy encompassing doors with diverse categories, geometries and mechanisms, is crucial for future embodied agents to effectively work in complex and broad real-world scenarios. Due to the limited datasets and unrealistic simulation environments, previous works fail to achieve good performance across various doors. In this work, we build a novel door manipulation environment reflecting different realistic door manipulation mechanisms, and further equip this environment with a large-scale door dataset covering 6 door categories with hundreds of door bodies and handles, making up thousands of different door instances. Additionally, to better emulate real-world scenarios, we introduce a mobile robot as the agent and use the partial and occluded point cloud as the observation, which are not considered in previous works while possessing significance for real-world implementations. To learn a universal policy over diverse doors, we propose a novel framework disentangling the whole manipulation process into three stages, and integrating them by training in the reversed order of inference. Extensive experiments validate the effectiveness of our designs and demonstrate our framework's strong performance. Code, data and videos are avaible on https://unidoormanip.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\Y8MBKEK7\Li 等 - 2024 - UniDoorManip Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipula.pdf}
}

@misc{liuRDT1BDiffusionFoundation2024,
  title = {{{RDT-1B}}: A {{Diffusion Foundation Model}} for {{Bimanual Manipulation}}},
  shorttitle = {{{RDT-1B}}},
  author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.07864},
  eprint = {2410.07864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zeroshot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{$\sim$}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to the project page for the code and videos.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\45HUUMGW\Liu 等 - 2024 - RDT-1B a Diffusion Foundation Model for Bimanual Manipulation.pdf}
}

@misc{liuRDT1BDiffusionFoundation2024a,
  title = {{{RDT-1B}}: A {{Diffusion Foundation Model}} for {{Bimanual Manipulation}}},
  shorttitle = {{{RDT-1B}}},
  author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.07864},
  eprint = {2410.07864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zeroshot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{$\sim$}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to the project page for the code and videos.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8HSGCM8T\Liu 等 - 2024 - RDT-1B a Diffusion Foundation Model for Bimanual Manipulation.pdf}
}

@incollection{lukacsFaithfulLeastsquaresFitting1998,
  title = {Faithful Least-Squares Fitting of Spheres, Cylinders, Cones and Tori for Reliable Segmentation},
  booktitle = {Computer {{Vision}} --- {{ECCV}}'98},
  author = {Luk{\'a}cs, Gabor and Martin, Ralph and Marshall, Dave},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Burkhardt, Hans and Neumann, Bernd},
  year = {1998},
  volume = {1406},
  pages = {671--686},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0055697},
  urldate = {2025-01-06},
  abstract = {This paper addresses a problem arising in the reverse engineering of solid models from depth-maps. We wish to identify and fit surfaces of known type wherever these are a good fit. This paper presents a set of methods for the least-squares fitting of spheres, cylinders, cones and tori to three-dimensional point data. Least-squares fitting of surfaces other planes, even of simple geometric type, has been little studied.},
  isbn = {978-3-540-64569-6 978-3-540-69354-3},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\XGJZQUUS\Lukács 等 - 1998 - Faithful least-squares fitting of spheres, cylinders, cones and tori for reliable segmentation.pdf}
}

@misc{lykovLLMBRAInAIdrivenFast2023,
  title = {{{LLM-BRAIn}}: {{AI-driven Fast Generation}} of {{Robot Behaviour Tree}} Based on {{Large Language Model}}},
  shorttitle = {{{LLM-BRAIn}}},
  author = {Lykov, Artem and Tsetserukou, Dzmitry},
  year = {2023},
  month = may,
  number = {arXiv:2305.19352},
  eprint = {2305.19352},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {This paper presents a novel approach in autonomous robot control, named LLMBRAIn, that makes possible robot behavior generation, based on operator's commands. LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description. We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003. The developed model accurately builds complex robot behavior while remaining small enough to be run on the robot's onboard microcomputer. The model gives structural and logical correct BTs and can successfully manage instructions that were not presented in training set. The experiment did not reveal any significant subjective differences between BTs generated by LLM-BRAIn and those created by humans (on average, participants were able to correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in only 4.53 out of 10 cases, indicating that their performance was close to random chance). The proposed approach potentially can be applied to mobile robotics, drone operation, robot manipulator systems and Industry 4.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\IIXVTEBS\\Lykov和Tsetserukou - 2023 - LLM-BRAIn AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\SLXM96A9\\2305.19352v1 (1)-zh.pdf}
}

@misc{lykovLLMBRAInAIdrivenFast2023a,
  title = {{{LLM-BRAIn}}: {{AI-driven Fast Generation}} of {{Robot Behaviour Tree}} Based on {{Large Language Model}}},
  shorttitle = {{{LLM-BRAIn}}},
  author = {Lykov, Artem and Tsetserukou, Dzmitry},
  year = {2023},
  month = may,
  number = {arXiv:2305.19352},
  eprint = {2305.19352},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19352},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach in autonomous robot control, named LLMBRAIn, that makes possible robot behavior generation, based on operator's commands. LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description. We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003. The developed model accurately builds complex robot behavior while remaining small enough to be run on the robot's onboard microcomputer. The model gives structural and logical correct BTs and can successfully manage instructions that were not presented in training set. The experiment did not reveal any significant subjective differences between BTs generated by LLM-BRAIn and those created by humans (on average, participants were able to correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in only 4.53 out of 10 cases, indicating that their performance was close to random chance). The proposed approach potentially can be applied to mobile robotics, drone operation, robot manipulator systems and Industry 4.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\5Y6PLRVC\Lykov和Tsetserukou - 2023 - LLM-BRAIn AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model.pdf}
}

@article{maiLLMRoboticBrain,
  title = {{{LLM}} as {{A Robotic Brain}}: {{Unifying Egocentric Memory}} and {{Control}}},
  author = {Mai, Jinjie and Chen, Jun and Li, Bing and Qian, Guocheng and Elhoseiny, Mohamed and Ghanem, Bernard},
  abstract = {Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLMBrain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\LPYGFKP3\Mai 等 - LLM as A Robotic Brain Unifying Egocentric Memory and Control.pdf}
}

@misc{majumdarImprovingVisionLanguageNavigation2020,
  title = {Improving {{Vision-and-Language Navigation}} with {{Image-Text Pairs}} from the {{Web}}},
  author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = may,
  number = {arXiv:2004.14973},
  eprint = {2004.14973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Following a navigation instruction such as `Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g.`stairs') to visual content in the environment (pixels corresponding to `stairs').},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\IAK6CMND\2004.14973v2.pdf}
}

@book{MeisheldonrossZhu;ZhaoXuanMinDengYi.GaiLuLunJiChuJiaoCheng2006,
  title = {{概率论基础教程}},
  author = {{(美)Sheldon Ross著 ; 赵选民等译.} and {罗斯.} and {赵选民.}},
  year = {2006},
  publisher = {机械工业出版社},
  address = {Bei jing},
  abstract = {本书系统介绍了概率论的基础理论及应用, 主要内容包括:组合分析, 概率论的公理, 条件概率与独立性, 随机变量及其分布, 数学期望, 极限定理, 随机模拟等},
  isbn = {978-7-111-18378-5},
  langid = {chinese},
  annotation = {OCLC: 303906169},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\I38BHNH5\\(美)Sheldon Ross著 \; 赵选民等译. 等 - 2006 - 概率论基础教程.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\J279ZP9E\\a_first_course_in_probability.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\MKFYM42Y\\A_Second_Course_in_Probability-Ross-Pekoz.pdf}
}

@article{mihaylovnaCYLINDRICALFUNCTIONSTHEIR,
  title = {{{CYLINDRICAL FUNCTIONS AND THEIR APPLICATIONS TO SOLVING PROBLEMS OF MATHEMATICAL PHYSICS}}},
  author = {Mihaylovna, Bogdan Anna},
  abstract = {The article is devoted to the study of cylindrical functions and their application in problems of mathematical physics. Cylindrical functions are widely used in modeling physical processes in cylindrical regions, such as wave propagation, thermal conductivity, and electromagnetic fields. The paper discusses the main types of cylindrical functions, including Bessel functions, modified Bessel functions and other special functions. The differential equations defining cylindrical functions and their basic properties are studied. Examples are given of the use of cylindrical functions to solve the Laplace equation, the Helmholtz equation and the heat equation in cylindrical coordinates. Particular attention is paid to the issues of numerical modeling of cylindrical functions and the features of their implementation in software. Thus, this work represents a comprehensive study of cylindrical functions and their use in problems of mathematical physics.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\5CHX77JD\Mihaylovna - CYLINDRICAL FUNCTIONS AND THEIR APPLICATIONS TO SOLVING PROBLEMS OF MATHEMATICAL PHYSICS.pdf}
}

@article{NovelHighPrecise2020,
  title = {A Novel High Precise Laser {{3D}} Profile Scanning Method with Flexible Calibration},
  year = {2020},
  month = sep,
  journal = {Optics and Lasers in Engineering},
  volume = {132},
  pages = {105938},
  publisher = {Elsevier},
  issn = {0143-8166},
  doi = {10.1016/j.optlaseng.2019.105938},
  urldate = {2024-12-08},
  abstract = {To improve the accuracy of laser-contour profile devices, we propose a 3D laser scanning system with high accuracy that includes camera and laser plan{\dots}},
  langid = {american},
  file = {C:\Users\guaoxiang\Zotero\storage\8VLS6J7B\S0143816619310784.html}
}

@article{nurunnabiRobustCylinderFitting,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1 m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84 m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1 m with only 0.27{$^\circ$} bias angle in the principal axis.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\AHWETT86\Nurunnabi - Robust cylinder fitting in laser scanning point cloud data.pdf}
}

@article{nurunnabiROBUSTCYLINDERFITTING2017,
  title = {{{ROBUST CYLINDER FITTING IN THREE-DIMENSIONAL POINT CLOUD DATA}}},
  author = {Nurunnabi, Abdul and Sadahiro, Yukio and Lindenbergh, Roderik},
  year = {2017},
  abstract = {This paper investigates the problems of cylinder fitting in laser scanning three-dimensional Point Cloud Data (PCD). Most existing methods require full cylinder data, do not study the presence of outliers, and are not statistically robust. But especially mobile laser scanning often has incomplete data, as street poles for example are only scanned from the road. Moreover, existence of outliers is common. Outliers may occur as random or systematic errors, and may be scattered and/or clustered. In this paper, we present a statistically robust cylinder fitting algorithm for PCD that combines Robust Principal Component Analysis (RPCA) with robust regression. Robust principal components as obtained by RPCA allow estimating cylinder directions more accurately, and an existing efficient circle fitting algorithm following robust regression principles, properly fit cylinder. We demonstrate the performance of the proposed method on artificial and real PCD. Results show that the proposed method provides more accurate and robust results: (i) in the presence of noise and high percentage of outliers, (ii) for incomplete as well as complete data, (iii) for small and large number of points, and (iv) for different sizes of radius. On 1000 simulated quarter cylinders of 1m radius with 10\% outliers a PCA based method fit cylinders with a radius of on average 3.63meter (m); the proposed method on the other hand fit cylinders of on average 1.02 m radius. The algorithm has potential in applications such as fitting cylindrical (e.g., light and traffic) poles, diameter at breast height estimation for trees, and building and bridge information modelling.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\9UXUVVMK\\translated_isprs-archives-XLII-1-W1-63-2017.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\HTBHJAPY\\Nurunnabi 等 - 2017 - ROBUST CYLINDER FITTING IN THREE-DIMENSIONAL POINT CLOUD DATA.pdf}
}

@article{nurunnabiRobustCylinderFitting2019,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul and Sadahiro, Yukio and Lindenbergh, Roderik and Belton, David},
  year = {2019},
  month = may,
  journal = {Measurement},
  volume = {138},
  pages = {632--651},
  issn = {0263-2241},
  doi = {10.1016/j.measurement.2019.01.095},
  urldate = {2024-11-30},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1\,m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84\,m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1\,m with only 0.27{$^\circ$} bias angle in the principal axis.},
  keywords = {3D modelling,Feature extraction,Robust measurement,Robust PCA,Robust regression,Shape reconstruction},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\W2IEMDV3\\Nurunnabi 等 - 2019 - Robust cylinder fitting in laser scanning point cloud data.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\EGYMY7W7\\S0263224119301046.html}
}

@article{nurunnabiRobustCylinderFittinga,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1 m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84 m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1 m with only 0.27{$^\circ$} bias angle in the principal axis.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\MJW9LUEV\Nurunnabi - Robust cylinder fitting in laser scanning point cloud data.pdf}
}

@article{nurunnabiRobustStatisticalApproaches2014,
  title = {Robust Statistical Approaches for Local Planar Surface Fitting in {{3D}} Laser Scanning Data},
  author = {Nurunnabi, Abdul and Belton, David and West, Geoff},
  year = {2014},
  month = oct,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {96},
  pages = {106--122},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2014.07.004},
  urldate = {2024-12-28},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\BB53UCMZ\Nurunnabi 等 - 2014 - Robust statistical approaches for local planar surface fitting in 3D laser scanning data.pdf}
}

@misc{PDFREDEEndEnd,
  title = {[{{PDF}}] {{REDE}}: {{End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}} {\textbar} {{Semantic Scholar}}},
  urldate = {2024-11-01},
  howpublished = {https://www.semanticscholar.org/paper/REDE\%3A-End-to-End-Object-6D-Pose-Robust-Estimation-Hua-Zhou/20f751603254a524c9b2de6c7076474e2260b945},
  keywords = {Pose},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\TQU6QK9R\\[PDF] REDE End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination .pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\WSPJGHV3\\20f751603254a524c9b2de6c7076474e2260b945.html}
}

@article{petersenHttpMatrixcookbookcom,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\TNN8BLHN\Petersen和Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{petersenHttpMatrixcookbookcoma,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\6LG6R8QZ\Petersen和Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{petersenHttpMatrixcookbookcomb,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english}
}

@article{prasadElliFitUnconstrainedNoniterative2013,
  title = {{ElliFit: An unconstrained, non-iterative, least squares based geometric Ellipse Fitting method}},
  shorttitle = {{ElliFit}},
  author = {Prasad, Dilip K. and Leung, Maylor K.H. and Quek, Chai},
  year = {2013},
  month = may,
  journal = {Pattern Recognition},
  volume = {46},
  number = {5},
  pages = {1449--1465},
  issn = {00313203},
  doi = {10.1016/j.patcog.2012.11.007},
  urldate = {2024-11-26},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\VVU957RS\Prasad 等 - 2013 - ElliFit An unconstrained, non-iterative, least squares based geometric Ellipse Fitting method.pdf}
}

@incollection{prasadPreciseEllipseFitting2012,
  title = {A {{Precise Ellipse Fitting Method}} for {{Noisy Data}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Prasad, Dilip K. and Quek, Chai and Leung, Maylor K. H.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Campilho, Aur{\'e}lio and Kamel, Mohamed},
  year = {2012},
  volume = {7324},
  pages = {253--260},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-31295-3_30},
  urldate = {2024-11-28},
  abstract = {Least squares based ellipse detection is used as a core process in many image processing applications. In order to restrict the solution to ellipses and avoid non-elliptic conics, constrained optimization has to be incorporated in the least squares model. This paper proposes a least squares method that does not require a constrained optimization and has very low false positive rates. In contrast to the algebraic model of conics, we use the geometric model of ellipse and minimize the geometric distance of the fitted ellipse from the digital curve. As a result, the solutions are strictly restricted to ellipses. Results demonstrate a superior performance than most least squares based method for elliptic curves and greater true negative rates for non-elliptic curves even in presence of 30\% noise.},
  isbn = {978-3-642-31294-6 978-3-642-31295-3},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\V3KN7R2S\\translated_A-Precise-Ellipse-Fitting-Method-for-Noisy-Data.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\YYMTYA8X\\Prasad 等 - 2012 - A Precise Ellipse Fitting Method for Noisy Data.pdf}
}

@misc{puigVirtualHomeSimulatingHousehold2018,
  title = {{{VirtualHome}}: {{Simulating Household Activities}} via {{Programs}}},
  shorttitle = {{{VirtualHome}}},
  author = {Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  year = {2018},
  month = jun,
  number = {arXiv:1806.07011},
  eprint = {1806.07011},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to ``drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\KW24T4NN\Puig 等 - 2018 - VirtualHome Simulating Household Activities via Programs.pdf}
}

@misc{radfordRobustSpeechRecognition2022,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04356},
  eprint = {2212.04356},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.04356},
  urldate = {2024-12-03},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\guaoxiang\Zotero\storage\HWJ84ZZK\Radford 等 - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@article{ranaSayPlanGroundingLarge,
  title = {{{SayPlan}}: {{Grounding Large Language Models}} Using {{3D Scene Graphs}} for {{Scalable Robot Task Planning}}},
  author = {Rana, Krishan and Haviland, Jesse and Garg, Sourav and {Abou-Chakra}, Jad and Reid, Ian},
  abstract = {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page sayplan.github.io.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\M4UK2RU8\Rana 等 - SayPlan Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning.pdf}
}

@misc{shahLMNavRoboticNavigation2022,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04429},
  eprint = {2207.04429},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\FRXT5V7X\Shah 等 - 2022 - LM-Nav Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.pdf}
}

@article{shakarjiLeastsquaresFittingAlgorithms1998,
  title = {Least-Squares Fitting Algorithms of the {{NIST}} Algorithm Testing System},
  author = {Shakarji, C.M.},
  year = {1998},
  month = nov,
  journal = {Journal of Research of the National Institute of Standards and Technology},
  volume = {103},
  number = {6},
  pages = {633},
  issn = {1044677X},
  doi = {10.6028/jres.103.043},
  urldate = {2024-11-08},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ME9FHGNG\Shakarji - 1998 - Least-squares fitting algorithms of the NIST algorithm testing system.pdf}
}

@misc{SoraFoundationRobots,
  title = {Sora for Foundation Robots with Parallel Intelligence: Three World Models, Three Robotic Systems {\textbar} {{Frontiers}} of {{Information Technology}} \& {{Electronic Engineering}}},
  urldate = {2024-11-17},
  howpublished = {https://link.springer.com/article/10.1631/FITEE.2400144},
  keywords = {sora},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6EMXWWYP\\Sora for foundation robots with parallel intelligence three world models, three robotic systems  F.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\66HLZCE2\\FITEE.html}
}

@article{strangArtLinearAlgebra2024,
  title = {{The Art of Linear Algebra}},
  author = {Strang, Gilbert and Hiranabe, Kenji and Fernandes, Ashley},
  year = {2024},
  month = mar,
  journal = {PRIMUS},
  pages = {1--14},
  issn = {1051-1970, 1935-4053},
  doi = {10.1080/10511970.2024.2321349},
  urldate = {2024-11-09},
  abstract = {我尝试为 Gilbert Strang 在书籍``Linear Algebra for Everyone''中介绍的矩阵的重要概念进行可视化图 释, 以促进从矩阵分解的角度对向量、矩阵计算和算法的理解. 1 它们包括矩阵分解 (Column-Row, CR)、高 斯消去法 (Gaussian Elimination, LU )、格拉姆-施密特正交化 (Gram-Schmidt Orthogonalization, QR)、特 征值和对角化 (Eigenvalues and Diagonalization, Q{$\Lambda$}QT)、和奇异值分解 (Singular Value Decomposition, U {$\Sigma$}V T).},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\2TSUQVIV\Strang 等 - 2024 - The Art of Linear Algebra.pdf}
}

@article{strangLinearAlgebraIts,
  title = {Linear {{Algebra}} and {{Its Applications}}},
  author = {Strang, Gilbert},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\667AGVU5\Strang - Linear Algebra and Its Applications.pdf}
}

@article{SymPyDocumentation,
  title = {{{SymPy Documentation}}},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\752UU7SA\SymPy Documentation.pdf}
}

@misc{teamOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  year = {2024},
  month = may,
  number = {arXiv:2405.12213},
  eprint = {2405.12213},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\5383YKA9\Team 等 - 2024 - Octo An Open-Source Generalist Robot Policy.pdf}
}

@misc{tianRoboKeyGenRobotPose2024,
  title = {{{RoboKeyGen}}: {{Robot Pose}} and {{Joint Angles Estimation}} via {{Diffusion-based 3D Keypoint Generation}}},
  shorttitle = {{{RoboKeyGen}}},
  author = {Tian, Yang and Zhang, Jiyao and Huang, Guowei and Wang, Bin and Wang, Ping and Pang, Jiangmiao and Dong, Hao},
  year = {2024},
  month = mar,
  number = {arXiv:2403.18259},
  eprint = {2403.18259},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render\&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render\&compare method and achieves higher inference speed. Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\QMVHIEZ5\Tian 等 - 2024 - RoboKeyGen Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation.pdf}
}

@misc{touraniVisualSLAMWhat2022,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10491},
  eprint = {2210.10491},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of forty-five impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\J4ZSVYR4\\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\NCI8ZZCC\\2210.html}
}

@article{touraniVisualSLAMWhat2022a,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = nov,
  journal = {Sensors},
  volume = {22},
  number = {23},
  eprint = {2210.10491},
  primaryclass = {cs},
  pages = {9297},
  issn = {1424-8220},
  doi = {10.3390/s22239297},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of fortyfive impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\K574BRR3\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf}
}

@misc{TutorialGraphBasedSLAM,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-10-30},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\Z5EKTX2J\5681215.html}
}

@misc{TutorialGraphBasedSLAMa,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}}},
  urldate = {2024-10-30},
  abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based SLAM problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the SLAM problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  langid = {american},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\GQ3HB9FH\A Tutorial on Graph-Based SLAM.pdf}
}

@misc{vempralaChatGPTRoboticsDesign2023,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\G4XHY63Q\\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\6M95KQZT\\2306.html}
}

@misc{vempralaChatGPTRoboticsDesign2023a,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\9JWJV8QA\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf}
}

@article{WANG20032953,
  title = {Using Symmetry in Robust Model Fitting},
  author = {Wang, Hanzi and Suter, David},
  year = {2003},
  journal = {Pattern Recognition Letters},
  volume = {24},
  number = {16},
  pages = {2953--2966},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(03)00156-9},
  abstract = {The pattern recognition and computer vision communities often employ robust methods for model fitting. In particular, high breakdown-point methods such as least median of squares (LMedS) and least trimmed squares (LTS) have often been used in situations where the data are contaminated with outliers. However, though the breakdown point of these methods can be as high as 50},
  keywords = {Breakdown point,Clustered outliers,Robust regression,Symmetry distance}
}

@article{Wang3DJiGuangLunKuoSaoMiaoYiZaiMuZhiBuJianBiaoMianCuCaoDuCeDingZhongDeYingYong2018,
  title = {{3D激光轮廓扫描仪在木制部件表面粗糙度测定中的应用}},
  author = {王, 莹莹 and 王, 珊珊 and 唐, 朝发},
  year = {2018},
  journal = {林产工业},
  volume = {45},
  number = {6},
  pages = {41--43},
  issn = {1001-5299},
  doi = {10.19531/j.issn1001-5299.201806010},
  urldate = {2024-12-08},
  abstract = {3D激光轮廓扫描仪是由一个线激光扫描仪从产品表面扫过,形成三维立体图像,可以检测扫描区域的各个点高度、体积、截面积、瑕疵等;通过对3D激光轮廓扫描仪获得的3D扫描图像,运用图像处理技术从光切显微图像中提取出表面轮廓断面图形,计算出木制部件表面粗糙度;采用3D激光轮廓扫描仪检测木制部件表面粗糙度,相对于触针式粗糙度检测仪,其检测受木材自身构造影响小,数据易保存,一经扫描完成,便可以反复测量、分析。},
  langid = {chinese},
  keywords = {,3D},
  file = {C:\Users\guaoxiang\Zotero\storage\XBZQFLAP\王 等 - 2018 - 3D激光轮廓扫描仪在木制部件表面粗糙度测定中的应用.pdf}
}

@misc{wangScalingProprioceptiveVisualLearning2024,
  title = {Scaling {{Proprioceptive-Visual Learning}} with {{Heterogeneous Pre-trained Transformers}}},
  author = {Wang, Lirui and Chen, Xinlei and Zhao, Jialiang and He, Kaiming},
  year = {2024},
  month = sep,
  number = {arXiv:2409.20537},
  eprint = {2409.20537},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.20537},
  urldate = {2024-12-03},
  abstract = {One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pretraining on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20\% on unseen tasks in multiple simulator benchmarks and real-world settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AM9IAWAL\Wang 等 - 2024 - Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers.pdf}
}

@article{wangYOLOv8PoseBoostAdvancementsMultimodal2024,
  title = {{{YOLOv8-PoseBoost}}: {{Advancements}} in {{Multimodal Robot Pose Keypoint Detection}}},
  shorttitle = {{{YOLOv8-PoseBoost}}},
  author = {Wang, Feng and Wang, Gang and Lu, Baoli},
  year = {2024},
  month = mar,
  journal = {Electronics},
  volume = {13},
  number = {6},
  pages = {1046},
  issn = {2079-9292},
  doi = {10.3390/electronics13061046},
  urldate = {2024-10-21},
  abstract = {In the field of multimodal robotics, achieving comprehensive and accurate perception of the surrounding environment is a highly sought-after objective. However, current methods still have limitations in motion keypoint detection, especially in scenarios involving small target detection and complex scenes. To address these challenges, we propose an innovative approach known as YOLOv8PoseBoost. This method introduces the Channel Attention Module (CBAM) to enhance the network's focus on small targets, thereby increasing sensitivity to small target individuals. Additionally, we employ multiple scale detection heads, enabling the algorithm to comprehensively detect individuals of varying sizes in images. The incorporation of cross-level connectivity channels further enhances the fusion of features between shallow and deep networks, reducing the rate of missed detections for small target individuals. We also introduce a Scale Invariant Intersection over Union (SIoU) redefined bounding box regression localization loss function, which accelerates model training convergence and improves detection accuracy. Through a series of experiments, we validate YOLOv8-PoseBoost's outstanding performance in motion keypoint detection for small targets and complex scenes. This innovative approach provides an effective solution for enhancing the perception and execution capabilities of multimodal robots. It has the potential to drive the development of multimodal robots across various application domains, holding both theoretical and practical significance.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\8JAC355V\Wang 等 - 2024 - YOLOv8-PoseBoost Advancements in Multimodal Robot Pose Keypoint Detection.pdf}
}

@article{wuNovelHighPrecise2020,
  title = {A Novel High Precise Laser {{3D}} Profile Scanning Method with Flexible Calibration},
  author = {Wu, Xiaojun and Tang, Na and Liu, Bo and Long, Zhili},
  year = {2020},
  month = sep,
  journal = {Optics and Lasers in Engineering},
  volume = {132},
  pages = {105938},
  issn = {01438166},
  doi = {10.1016/j.optlaseng.2019.105938},
  urldate = {2024-12-08},
  abstract = {To improve the accuracy of laser-contour profile devices, we propose a 3D laser scanning system with high accuracy that includes camera and laser plane calibration, and laser-stripe center extraction. First, a calibration device was designed and manufactured, from which the fiducial points can be easily and accurately detected to calibrate the laser plane. Then through the perspective invariance and the projective invariant cross-ratio properties, a flexible and accurate calibration algorithm is presented. A polynomial fitting method is employed to overcome the discontinuity and thickness of the laser stripe center. Finally, a cubic spline interpolation is utilized to restore the continuous gray value of the transitional regions of the light stripe. Experimental results with different test pieces demonstrate that the minimum geometric accuracy is 18{$\mu$}m and the repeatability is less than {\textpm} 3{$\mu$}m, which indicates that the proposed method is feasible to be used in industrial high precision measurement.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\VPI2REJ5\Wu 等 - 2020 - A novel high precise laser 3D profile scanning method with flexible calibration.pdf}
}

@misc{wuUnleashingLargeScaleVideo2023,
  title = {Unleashing {{Large-Scale Video Generative Pre-training}} for {{Visual Robot Manipulation}}},
  author = {Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13139},
  eprint = {2312.13139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.13139},
  urldate = {2024-12-03},
  abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9\% to 94.9\%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3\% to 85.4\%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\2SWRSSQL\Wu 等 - 2023 - Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation.pdf}
}

@misc{xuFASTLIOFastRobust2021,
  title = {{{FAST-LIO}}: {{A Fast}}, {{Robust LiDAR-inertial Odometry Package}} by {{Tightly-Coupled Iterated Kalman Filter}}},
  shorttitle = {{{FAST-LIO}}},
  author = {Xu, Wei and Zhang, Fu},
  year = {2021},
  month = apr,
  number = {arXiv:2010.08196},
  eprint = {2010.08196},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {This paper presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in real-time: running on a quadrotor onboard computer, it fuses more than 1,200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are open-sourced on Github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3UCVV3L8\\Xu和Zhang - 2021 - FAST-LIO A Fast, Robust LiDAR-inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\MR2J7BLA\\2010.html}
}

@misc{xuPIDNetRealtimeSemantic2023,
  title = {{{PIDNet}}: {{A Real-time Semantic Segmentation Network Inspired}} by {{PID Controllers}}},
  shorttitle = {{{PIDNet}}},
  author = {Xu, Jiacong and Xiong, Zixiang and Bhattacharyya, Shankar P.},
  year = {2023},
  month = apr,
  number = {arXiv:2206.02066},
  eprint = {2206.02066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-IntegralDerivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel threebranch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6\% mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1\% mIOU with speed of 153.7 FPS on CamVid.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\VCSCPDXZ\Xu 等 - 2023 - PIDNet A Real-time Semantic Segmentation Network Inspired by PID Controllers.pdf}
}

@book{YangJuZhenLun2003,
  title = {矩阵论},
  author = {杨, 明 and 刘, 先忠},
  year = {2003},
  edition = {第二版},
  publisher = {华中科技大学出版社},
  file = {C:\Users\guaoxiang\Zotero\storage\ITXWM2XQ\《矩阵论（第二版）》【杨明】.pdf}
}

@article{yuCuttingPlaneBased,
  title = {Cutting {{Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data}} for {{Digital Fringe Projection}}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  abstract = {Benefiting from the characteristics of full field scanning, high resolution and high precision, digital fringe projection measurement technology has been widely used in three-dimensional measurement. For the cylinder pose detection, due to the influence of occlusion and noise, the obtained three-dimensional point cloud information is usually incomplete, resulting in the unreliable object parameters after point cloud fitting. In view of the above problems, a cylinder fitting method with incomplete point cloud data based on the cutting plane is proposed. Firstly, the point cloud is processed by bilateral filtering method to reduce the influence of outliers and noise points. Then, according to the principle of forming ellipse on cylinder cutting plane, some equidistant cutting planes are selected. At the same time, in order to reduce the influence of outliers on fitting, the random sampling consistency algorithm is used for ellipse fitting with the points on each cutting plane, that is, the ellipse parameters corresponding to each cutting plane can be obtained. Finally, random sampling consistency algorithm is applied for cylinder axis fitting with the ellipse centers of all the cutting planes, calculating the axis vector of the cylinder. Thus, the parameters of the cylinder detected are obtained. The experimental results of stepped cylinder with diffuse reflection and metal cylinder show the effectiveness and high precision of the proposed method.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\92JSH3DZ\Yu 等 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@article{yuCuttingPlaneBased2020,
  title = {Cutting {{Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data}} for {{Digital Fringe Projection}}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {149385--149401},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3016424},
  urldate = {2024-11-25},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\KL9W45ZI\Yu 等 - 2020 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@article{yuCuttingPlaneBased2020a,
  title = {{Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Projection}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {149385--149401},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3016424},
  urldate = {2024-11-25},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\P6CQBWAT\Yu 等 - 2020 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@misc{zhangGAMMAGraspabilityAwareMobile2024,
  title = {{GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion}},
  shorttitle = {{GAMMA}},
  author = {Zhang, Jiazhao and Gireesh, Nandiraju and Wang, Jilong and Fang, Xiaomeng and Xu, Chaoyi and Chen, Weiguang and Dai, Liu and Wang, He},
  year = {2024},
  month = mar,
  number = {arXiv:2309.15459},
  eprint = {2309.15459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.15459},
  urldate = {2024-09-25},
  abstract = {Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses.},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\LQWSWFIX\\Zhang 等 - 2024 - GAMMA Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\9DUDT2VI\\2309.html}
}

@article{zhanImprovingEyeDisplay2019,
  title = {Improving Near-Eye Display Resolution by Polarization Multiplexing},
  author = {Zhan, Tao and Xiong, Jianghao and Tan, Guanjun and Lee, Yun-Han and Yang, Jilin and Liu, Sheng and Wu, Shin-Tson},
  year = {2019},
  month = may,
  journal = {Optics Express},
  volume = {27},
  number = {11},
  pages = {15327},
  issn = {1094-4087},
  doi = {10.1364/OE.27.015327},
  urldate = {2024-10-14},
  abstract = {We present here an optical approach to boost the apparent pixel density by utilizing the superimposition of two shifted-pixel grids generated by a Pancharatnam-Berry deflector (PBD). The content of the two shifted pixel grids are presented to the observer's eye simultaneously using a polarization-multiplexing method. Considering the compact and lightweight nature of PBD, this approach has potential applications in near-eye display systems. Moreover, the same concept can be extended to projection displays with proper modifications.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\BG8WLZ9Q\Zhan 等 - 2019 - Improving near-eye display resolution by polarization multiplexing.pdf}
}

@misc{zhaoAgentCerebrumController2023,
  title = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}: {{Implementing}} an {{Embodied LMM-based Agent}} on {{Drones}}},
  shorttitle = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}},
  author = {Zhao, Haoran and Pan, Fengxing and Ping, Huqiuyue and Zhou, Yaoming},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15033},
  eprint = {2311.15033},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this study, we present a novel paradigm for industrial robotic embodied agents, encapsulating an agent as cerebrum, controller as cerebellum architecture. Our approach harnesses the power of Large Multimodal Models (LMMs) within an agent framework known as AeroAgent, tailored for drone technology in industrial settings. To facilitate seamless integration with robotic systems, we introduce ROSchain, a bespoke linkage framework connecting LMM-based agents to the Robot Operating System (ROS). We report findings from extensive empirical research, including simulated experiments on the Airgen and real-world case study, particularly in individual search and rescue operations. The results demonstrate AeroAgent's superior performance in comparison to existing Deep Reinforcement Learning (DRL)-based agents, highlighting the advantages of the embodied LMM in complex, real-world scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\54QUDRMU\Zhao 等 - 2023 - Agent as Cerebrum, Controller as Cerebellum Implementing an Embodied LMM-based Agent on Drones.pdf}
}

@inproceedings{zhaoARassistedHumanRobotInteraction2024,
  title = {An {{AR-assisted Human-Robot Interaction System}} for {{Improving LLM-based Robot Control}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Cybernetics}} and {{Intelligent Systems}} ({{CIS}}) and {{IEEE International Conference}} on {{Robotics}}, {{Automation}} and {{Mechatronics}} ({{RAM}})},
  author = {Zhao, Ziyue and Lou, Shanhe and Tan, Runjia and Lv, Chen},
  year = {2024},
  month = aug,
  pages = {144--149},
  issn = {2326-8239},
  doi = {10.1109/CIS-RAM61939.2024.10673005},
  urldate = {2024-11-18},
  abstract = {The LLM-based robot control system enables robots to understand and execute high-level language instructions. However, the motion plans generated by the LLM-based robot control system are not always reliable. To allow operators to monitor the working state of the robot after issuing task instructions in high-level language, and demonstrate the robot's motion plan safely, we designed an AR-assisted human-robot interaction system called SeeIt. This system displays the working state of a robotic arm and predicts its motion trajectory regarding LLM output. Ambiguities in high-level language instructions and perception misunderstanding about the surrounding environment may lead to erroneous motion plans. The AR interaction system allows operators to choose among potential running trajectories and to adjust erroneous motion plans, leveraging human decision-making to enhance the system's intelligence. A case study was conducted to evaluate the usability and performance of this system. We expected that SeeIt will increase the interaction experience between operator and robot, improving the task performance of the LLM-based robot control system.},
  langid = {american},
  keywords = {argument reality,arm,digital twin,High level languages,human-robot interaction,Human-robot interaction,llm,motion planning,Planning,Random access memory,Reliability engineering,Robot control,Trajectory},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3HDFA2IX\\Zhao 等 - 2024 - An AR-assisted Human-Robot Interaction System for Improving LLM-based Robot Control.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\UMHF77U6\\10673005.html}
}

@misc{zhaoLearningFineGrainedBimanual2023,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13705},
  eprint = {2304.13705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  urldate = {2024-12-03},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8RWRW2SS\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf}
}

@article{zhaoRobustPrincipalComponent,
  title = {Robust {{Principal Component Analysis}} with {{Complex Noise}}},
  author = {Zhao, Qian and Meng, Deyu and Xu, Zongben and Zuo, Wangmeng and Zhang, Lei},
  abstract = {The research on robust principal component analysis (RPCA) has been attracting much attention recently. The original RPCA model assumes sparse noise, and use the L1-norm to characterize the error term. In practice, however, the noise is much more complex and it is not appropriate to simply use a certain Lp-norm for noise modeling. We propose a generative RPCA model under the Bayesian framework by modeling data noise as a mixture of Gaussians (MoG). The MoG is a universal approximator to continuous distributions and thus our model is able to fit a wide range of noises such as Laplacian, Gaussian, sparse noises and any combinations of them. A variational Bayes algorithm is presented to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed form. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and background subtraction.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\CUHE6NKY\Zhao 等 - Robust Principal Component Analysis with Complex Noise.pdf}
}

@inproceedings{zhengyouzhangFlexibleCameraCalibration1999,
  title = {Flexible Camera Calibration by Viewing a Plane from Unknown Orientations},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {{Zhengyou Zhang}},
  year = {1999},
  pages = {666-673 vol.1},
  publisher = {IEEE},
  address = {Kerkyra, Greece},
  doi = {10.1109/ICCV.1999.791289},
  urldate = {2024-11-11},
  abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use. The corresponding software is available from the author's Web page.},
  isbn = {978-0-7695-0164-2},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\XECWGQE4\Zhengyou Zhang - 1999 - Flexible camera calibration by viewing a plane from unknown orientations.pdf}
}
